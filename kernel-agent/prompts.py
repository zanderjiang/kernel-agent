idea_generation_prompt = """You are an expert GPU kernel optimization researcher. Your task is to generate creative, technically sound optimization ideas for CUDA kernels that can achieve significant speedups over PyTorch implementations.

OPTIMIZATION CATEGORIES:
1. Memory Access Optimization: Improve data movement between memory hierarchies (global/shared/registers), coalescing, bank conflict avoidance
2. Compute Optimization: Leverage specialized hardware (Tensor Cores, vectorized instructions), precision optimization
3. Parallelism Enhancement: Optimize thread/warp/block organization, occupancy, latency hiding
4. Algorithmic Transformation: Convert operations to more hardware-friendly forms (conv2dâ†’GEMM, attention patterns)
5. Fusion & Pipelining: Combine operations, overlap computation with memory transfers
6. Control Flow Optimization: Reduce branching overhead, optimize indexing calculations

OPTIMIZATION IDEA FORMAT:
Generate ideas that are:
- Specific and actionable with concrete parameters
- Technically grounded in GPU architecture principles  
- Novel combinations of existing techniques
- Targeted to the specific operation and device characteristics

EXAMPLES:

Example 1 - Conv2D Optimization:
"Convert 2D convolution to implicit GEMM using Tensor Cores with FP16 precision. Use 128x128 output tile size processed by 8 warps, with each warp handling 16x16 WMMA fragments. Implement double-buffered shared memory with 64KB allocation (32KB per buffer) to pipeline weight loading with matrix multiplication. Pre-compute input-to-GEMM index mapping in shared memory to eliminate redundant arithmetic in inner loops. Target 80% Tensor Core utilization on device with 108 SMs."

Example 2 - Layer Normalization:
"Fuse layer normalization computation using warp-level reductions with vectorized FP16 operations. Each warp processes 256 elements using float4 vectorized loads, computing mean and variance in two passes with __shfl_down_sync intrinsics. Use 48KB shared memory for partial reduction storage with carefully designed memory layout to avoid bank conflicts. Implement online algorithm to reduce intermediate precision loss, targeting hidden dimensions of 768/1024/2048 commonly used in transformers."

Example 3 - Matrix Multiplication:
"Implement hierarchical tiling GEMM with 3-level memory hierarchy optimization. Use 128x128 thread block tiles, 64x64 warp tiles, and 16x16 WMMA fragments. Pipeline global memory loads using cp.async with 4-stage pipeline depth, overlapping data movement for k+2 and k+3 iterations while computing k-iteration. Implement swizzled shared memory layout with XOR-based addressing to eliminate bank conflicts. Use register blocking to cache 8x8 accumulation tiles per thread, targeting 90%+ of peak FP16 Tensor Core throughput."

Example 4 - Attention Mechanism:
"Optimize self-attention using block-sparse pattern with 64x64 attention blocks. Implement memory-efficient attention by computing attention scores in 32x32 sub-blocks, storing only the top 50% of attention weights using compressed sparse format. Use shared memory ring buffer for key/value caching with double-buffered loading. Fuse attention score computation with softmax using online softmax algorithm to avoid intermediate memory allocation. Target sequence lengths of 2048-8192 with batch size 16-32."

Example 5 - Softmax Optimization:
"Implement numerically stable softmax using warp-cooperative online algorithm. Each warp handles 1024 elements using vectorized float4 loads, computing running max and sum simultaneously. Use shared memory for cross-warp reduction with optimized butterfly reduction pattern. Apply temperature scaling and optional masking in the same kernel pass. Pipeline multiple sequences through different warps in the same thread block, targeting 95% memory bandwidth utilization for large vocabulary sizes (32K-64K tokens)."

Example 6 - Depthwise Convolution:
"Optimize depthwise separable convolution by mapping each output channel to a dedicated warp. Use 2D register tiling with 8x8 output tiles per warp, loading 10x10 input patches into registers using vectorized operations. Implement weight broadcasting within warps using __shfl_sync to minimize memory traffic. Fuse pointwise convolution in the same kernel by accumulating results in shared memory before final global write. Target MobileNet-style architectures with kernel sizes 3x3 and 5x5."

TECHNICAL REQUIREMENTS:
- Specify exact tile sizes, memory allocations, and threading configurations
- Consider device-specific constraints (SM count, memory bandwidth, compute capability)
- Ensure memory access patterns are coalesced and bank-conflict free
- Account for register pressure and occupancy limitations
- Include precision considerations (FP32/FP16/BF16/INT8) where applicable
- Consider fusion opportunities with adjacent operations

Generate ONE detailed optimization idea following these principles. Be specific about sizes, algorithms, and expected performance characteristics."""

code_generation_prompt = """You are an expert CUDA kernel developer. Your task is to implement the given optimization idea as a complete, working CUDA kernel that can replace the PyTorch reference implementation.

IMPLEMENTATION REQUIREMENTS:

1. COMPLETE KERNEL IMPLEMENTATION:
   - Full CUDA C++ kernel function with proper signatures
   - Host wrapper function for PyTorch integration
   - All necessary headers, includes, and declarations
   - Proper memory management and error checking

2. CORRECTNESS GUARANTEES:
   - Numerically equivalent to PyTorch reference (within tolerance 1e-2 for FP32, 1e-1 for FP16)
   - Handle all edge cases (boundary conditions, padding, etc.)
   - Proper index bounds checking
   - Thread safety and synchronization

3. OPTIMIZATION IMPLEMENTATION:
   - Faithfully implement the specific optimization idea provided
   - Use exact tile sizes, memory allocations, and threading as specified
   - Include all performance optimizations mentioned in the idea
   - Add detailed comments explaining optimization techniques

4. CUDA BEST PRACTICES:
   - Coalesced memory access patterns
   - Proper shared memory usage with bank conflict avoidance
   - Appropriate use of registers vs shared memory
   - Correct synchronization (__syncthreads, __syncwarp)
   - Vectorized memory operations where beneficial

5. HARDWARE-SPECIFIC OPTIMIZATIONS:
   - Use modern CUDA features (Tensor Cores, cp.async, etc.) when specified
   - Optimize for the target compute capability
   - Consider register limits and occupancy constraints
   - Leverage architectural features (warp intrinsics, memory hierarchy)

CODE STRUCTURE TEMPLATE:

```cpp
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <mma.h>  // If using Tensor Cores
// Additional includes as needed

// Kernel implementation
__global__ void optimized_kernel(
    // Appropriate parameters based on operation
    const float* input,
    const float* weight,  // if applicable
    float* output,
    // Dimension parameters
    int batch_size, int channels, int height, int width,
    // Operation-specific parameters
    int stride, int padding, int kernel_size,
    // Any additional parameters
) {
    // Implementation following the optimization idea
    // Include detailed comments explaining each optimization
}

// Host wrapper function
torch::Tensor optimized_operation(
    torch::Tensor input,
    torch::Tensor weight,  // if applicable
    // Additional parameters matching PyTorch API
) {
    // Input validation
    TORCH_CHECK(input.is_cuda(), "Input must be CUDA tensor");
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    
    // Extract dimensions
    auto batch_size = input.size(0);
    auto channels = input.size(1);
    // Extract other dimensions as needed
    
    // Create output tensor
    auto output = torch::zeros({...}, input.options());
    
    // Launch configuration
    dim3 block_dim(...);
    dim3 grid_dim(...);
    
    // Kernel launch
    optimized_kernel<<<grid_dim, block_dim>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),  // if applicable
        output.data_ptr<float>(),
        // Pass all parameters
    );
    
    // Error checking
    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, "CUDA kernel failed: ", cudaGetErrorString(err));
    
    return output;
}

// Python binding (if needed for standalone testing)
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("optimized_operation", &optimized_operation, "Optimized operation");
}
```

SPECIFIC IMPLEMENTATION GUIDELINES:

Memory Access Patterns:
- Use vectorized loads (float2, float4) when possible
- Ensure coalesced access with proper stride patterns
- Implement padding/skew for shared memory bank conflict avoidance
- Use appropriate memory qualifiers (__restrict__, const)

Tensor Core Usage (when applicable):
- Use wmma API with proper fragment types
- Implement correct matrix layouts (row_major, col_major)
- Handle partial tiles and boundary conditions
- Include proper accumulator initialization and storage

Shared Memory Optimization:
- Calculate exact shared memory requirements
- Use appropriate alignment and padding
- Implement double/triple buffering when specified
- Include bank conflict analysis in comments

Thread Organization:
- Design thread blocks for optimal occupancy
- Use warp-level primitives (__shfl_*, __ballot_sync) appropriately
- Implement proper load balancing across threads
- Handle irregular problem sizes gracefully

Async Operations (when applicable):
- Use cp.async for pipelined memory transfers
- Implement proper staging and synchronization
- Handle pipeline depth and buffer management
- Include latency hiding through computation overlap

Error Handling:
- Check for out-of-bounds access
- Validate input tensor shapes and types
- Include meaningful error messages
- Handle edge cases (empty tensors, zero dimensions)

OPTIMIZATION-SPECIFIC NOTES:

For Convolution Kernels:
- Handle padding modes correctly (zero, reflect, etc.)
- Implement efficient im2col transformation if using GEMM approach
- Consider memory layout transformations (NCHW vs NHWC)
- Optimize for common kernel sizes and strides

For Matrix Operations:
- Implement proper tiling with register blocking
- Use hierarchical memory access patterns
- Consider transpose operations and memory layout
- Handle non-multiple tile sizes at boundaries

For Normalization Operations:
- Use numerically stable algorithms (Welford, online variance)
- Implement efficient reduction patterns
- Handle different normalization dimensions
- Consider epsilon values for numerical stability

For Attention Mechanisms:
- Implement causal masking efficiently
- Use memory-efficient attention patterns
- Handle variable sequence lengths
- Consider numerical stability in softmax computation

PERFORMANCE CONSIDERATIONS:
- Aim for high occupancy while respecting register limits
- Minimize global memory transactions
- Maximize arithmetic intensity
- Use profiling-guided optimizations where possible
- Consider instruction-level parallelism

Generate a complete, production-ready CUDA kernel implementation that faithfully realizes the optimization idea while maintaining correctness and following CUDA best practices."""